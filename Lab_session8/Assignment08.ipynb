{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c60732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\santo\\\\OneDrive - Amrita vishwa vidyapeetham\\\\Documents\\\\4th sem Btech\\\\Machine Learning\\\\datasets\\\\question_answers_answerkey.xlsx\"\n",
    "df = pd.read_excel(file_path, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c59d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(column):\n",
    "    \"\"\"Calculate the entropy of a DataFrame column.\"\"\"\n",
    "    probabilities = column.value_counts(normalize=True)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e826d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conditional_entropy(feature_column, target_column):\n",
    "    \"\"\"Calculate the conditional entropy of a feature given the target.\"\"\"\n",
    "    # Calculate the probabilities of each feature category\n",
    "    probabilities = feature_column.value_counts(normalize=True)\n",
    "    conditional_entropy = 0\n",
    "    \n",
    "    # Iterate over each category in the feature column\n",
    "    for category in feature_column.unique():\n",
    "        # Filter the target column based on the feature category\n",
    "        target_subset = target_column[feature_column == category]\n",
    "        # Calculate and sum the weighted entropy for the category\n",
    "        conditional_entropy += probabilities[category] * calculate_entropy(target_subset)\n",
    "    \n",
    "    return conditional_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d755bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root_node(df, target_column_name):\n",
    "    \"\"\"Find the best feature to use as the root node based on Information Gain.\"\"\"\n",
    "    target_column = df[target_column_name]\n",
    "    initial_entropy = calculate_entropy(target_column)\n",
    "    max_information_gain = -1\n",
    "    best_feature = None\n",
    "    \n",
    "    for feature in df.drop(columns=[target_column_name]).columns:\n",
    "        feature_column = df[feature]\n",
    "        conditional_entropy = calculate_conditional_entropy(feature_column, target_column)\n",
    "        information_gain = initial_entropy - conditional_entropy\n",
    "        \n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            best_feature = feature\n",
    "    \n",
    "    return best_feature, max_information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73561f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best root node feature is: 1.0 with an information gain of: 1.3746884964855104\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame and 'target' is the name of the target column\n",
    "# Example usage:\n",
    "best_feature, information_gain = find_root_node(df, 3.5)\n",
    "print(f\"The best root node feature is: {best_feature} with an information gain of: {information_gain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35edc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2\n",
      "1      1\n",
      "2      2\n",
      "3      2\n",
      "4      1\n",
      "      ..\n",
      "195    1\n",
      "196    0\n",
      "197    1\n",
      "198    1\n",
      "199    1\n",
      "Name: binned_A1, Length: 200, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bin_feature(series, binning_type='equal_width', n_bins=3):\n",
    "    if binning_type == 'equal_width':\n",
    "        # Equal width binning\n",
    "        bins = np.linspace(series.min(), series.max(), n_bins + 1)\n",
    "        return pd.cut(series, bins=bins, labels=False, include_lowest=True)\n",
    "    \n",
    "    elif binning_type == 'equal_freq':\n",
    "        # Equal frequency binning (quantile-based)\n",
    "        return pd.qcut(series, q=n_bins, labels=False, duplicates='drop')\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid binning_type. Choose 'equal_width' or 'equal_freq'.\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame and '1.0' is a continuous feature you want to bin\n",
    "feature = 1.0  # Example feature name\n",
    "binned_feature = bin_feature(df[feature], binning_type='equal_width', n_bins=3)\n",
    "df['binned_A1'] = binned_feature  # Adding the binned feature back to the DataFrame\n",
    "print(df['binned_A1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts)) * np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "def InfoGain(data, split_attribute_name, target_name=\"target\"):\n",
    "    # Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    # Calculate the values and the corresponding counts for the split attribute \n",
    "    vals, counts= np.unique(data[split_attribute_name], return_counts=True)\n",
    "    \n",
    "    # Calculate the weighted entropy\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts)) * entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    # Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, originaldata, features, target_attribute_name=\"target\", parent_node_class = None):\n",
    "    # If all target_values have the same value, return this value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    # If the dataset is empty, return the mode target feature value in the original dataset\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])]\n",
    "    \n",
    "    # If the feature space is empty, return the mode target feature value of the direct parent node\n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    # If none of the above conditions holds true, grow the tree!\n",
    "    else:\n",
    "        # Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "        \n",
    "        # Select the feature which best splits the dataset\n",
    "        item_values = [InfoGain(data, feature, target_attribute_name) for feature in features] # Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        # Create the tree structure. The root gets the name of the feature with maximum information gain\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        # Remove the feature with the best information gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        # Grow a branch under the root node for each possible value of the root node feature\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            # Split the dataset along the value of the feature with the largest information gain and create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            # Call the build_tree function for each of those sub_datasets with the new parameters\n",
    "            subtree = build_tree(sub_data, originaldata, features, target_attribute_name, parent_node_class)\n",
    "            \n",
    "            # Add the sub tree\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbed42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(data, target):\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    current_entropy = calculate_entropy(target)\n",
    "    \n",
    "    for feature in data.columns:\n",
    "        # Calculate the Information Gain for splitting on this feature\n",
    "        gain = information_gain(data, target, feature, current_entropy)\n",
    "        \n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature\n",
    "            \n",
    "    return best_feature, best_gain\n",
    "\n",
    "def information_gain(data, target, feature, current_entropy):\n",
    "    \"\"\"Calculate the Information Gain of dividing the data on the chosen feature.\"\"\"\n",
    "    unique_values = data[feature].unique()\n",
    "    conditional_entropy = 0\n",
    "    \n",
    "    for value in unique_values:\n",
    "        subset_target = target[data[feature] == value]\n",
    "        prob = len(subset_target) / len(target)\n",
    "        conditional_entropy += prob * calculate_entropy(subset_target)\n",
    "    \n",
    "    return current_entropy - conditional_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, target, feature):\n",
    "    \"\"\"Split the dataset and the target based on the best split feature.\"\"\"\n",
    "    unique_values = data[feature].unique()\n",
    "    splits = {}\n",
    "    \n",
    "    for value in unique_values:\n",
    "        split_mask = data[feature] == value\n",
    "        splits[value] = (data[split_mask], target[split_mask])\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, target, max_depth=3, depth=0):\n",
    "    if depth == max_depth or len(data.columns) == 0:\n",
    "        # Stopping condition met, return a leaf node\n",
    "        return DecisionNode(value=target.mode()[0])\n",
    "    \n",
    "    best_feature, best_gain = find_best_split(data, target)\n",
    "    if best_gain == 0:\n",
    "        # No feature provides a gain, return a leaf node\n",
    "        return DecisionNode(value=target.mode()[0])\n",
    "    \n",
    "    # Perform the split\n",
    "    splits = split_data(data, target, best_feature)\n",
    "    \n",
    "    # Create node and recurse for each split\n",
    "    node = DecisionNode(feature=best_feature)\n",
    "    node.children = {\n",
    "        value: build_tree(sub_data, sub_target, max_depth, depth+1)\n",
    "        for value, (sub_data, sub_target) in splits.items()\n",
    "    }\n",
    "    \n",
    "    return node\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
